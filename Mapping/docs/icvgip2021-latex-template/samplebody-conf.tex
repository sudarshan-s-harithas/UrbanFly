\section{Introduction}
Recent works have shown that TSDF (Truncated Signed Distance Function) based representation of a map can be used for path planning[voxblox, voxgraph papers] as well as to generate human-readable meshes. However, in constrained systems like an Micro Aerial Vehicle (MAV) equipped with a monocular camera and an IMU, obtaining dense point cloud needed for an SDF based planner is a challenge.

\section{Introduction}
Autonomous navigation of Unmanned Aerial Vehicles (UAVs), equipped with a single RGB sensor and IMU, in urban environments can be done by dense mapping of the skylines. The generated map is then converted to representations like octomap, TSDF, etc., for obstacle avoidance and path planning. Recent works have shown that TSDF (Truncated Signed Distance Function) based representation of a map can be used for path planning[voxblox, voxgraph papers] as well as to generate human-readable meshes. However, in constrained systems like UAVs, obtaining dense point clouds needed for computing TSDFs is a challenge.

Although, VIO (Visual-Inertial odometry) systems can perform state estimation accurately in real-time, they produce a sparse feature map. Existing methods that produce a dense map from monocular images are computationally expensive or donot consider structural regularities in the scene.

Reconstruction of skyline from monocular images, demands a mapping system that (i) produces a dense map (ii) is simple to compute (iii) can leverage the structural regularities of the scene. 

While there are numerous approaches for visual-inertial odometry we use VINS-Mono. VINS-Mono is a state-of-the-art, monocular visual-inertial odometry system that is based on a tightly-coupled sliding-window optimization of preintegrated IMU measurements and visual features. We use it for odometry and ignore its relocalization and loop-closure modules. We build upon its front-end to detect and track only planar features in the scene using the plane masks. Estimated camera poses are used to triangulate the 2D planar features to get sparse 3D features. Our mapping pipeline starts by estimating the breadth and height of the plane segment by fitting a vertical 3D bounding box. Resulting 3D bouding box is then rotated to align with the nearest vanishing point direction. This 3D bounding box, that represents the 3D plane segment, is used to directly compute the TSDF values. Finally we use voxblox (i) to compute the ESDF voxel map and (ii) to generate 3D mesh for visualization. For baseline experiments, the sparse planar feature cloud is used to compute the TSDF values.

\section{Related Work}

\section{Pipeline}
The proposed pipeline for the reconstruction of skyline from monocular UAV images can be divided into three steps viz., Preprocessing, Visual-Inertial Odometry, Estimation of geometry and TSDF computation. Each image frame, captured from the RGB sensor, thus triggers the these steps in the same order.

\begin{figure*}
  \centering
  \subcaptionbox{Preprocessing stage}{
    \includegraphics[width=0.22\linewidth, height=0.3\textheight]{images/pipeline/sample_masked.png}
  }
  \subcaptionbox{Visual-Inertial Odometry}{
    \includegraphics[width=0.22\linewidth, height=0.3\textheight]{images/pipeline/sample_features.png}
  }
  \subcaptionbox{Estimation of geometric parameters of plane segment}{
    \includegraphics[width=0.22\linewidth, height=0.3\textheight]{images/pipeline/sample_bbox.png}
  }
  \subcaptionbox{TSDF computation}{
    \includegraphics[width=0.22\linewidth, height=0.3\textheight]{images/pipeline/sample_sdfs.png}
  }
  \caption{Our approach consists of four stages. Figures (a)-(d) show the result of each stage involved in reconstructing SDF representation of a planar facade of one of the buildings in our dataset. Firstly, preprocessing stage performs plane instance segmentation and assigns the normal direction to each plane segment based on the vanishing points. (a) shows a plane mask overlaid onto the image and coloured based on its normal direction. VIO then uses a sliding-window approach to estimate the odometry of the latest frame. (b) shows the 3D features on the facade masked in (a). Plane segment parameters viz., height and breadth are estimated by fitting 3D bounding box to the features, as shown in (c). Each plane segment is then modelled as a thin cuboid with estimated dimensions. TSDF values are then computed using the distance function mentioned in section 3.3. As shown in (d) this TSDF voxel map can be used to generate a dense 3D mesh (shown in gray) and also to compute the ESDF values. The voxels are coloured based on their distance to the surface.}
\end{figure*}

Objectives of each step are as follows:
\begin{itemize}
    \item Preprocessing: To extract plane instance masks and vanishing point directions
    \item Visual Inertial Odometry: To compute the camera pose and to compute/update the depth estimates of the tracked planar features
    \item Estimation of geometry: To estimate/update the breadth, height and the orientation of plane segments with tracked planar features
    \item TSDF computation: To directly compute/update the TSDF/ESDF  voxel map from the estimated geometry and to generate meshes for visualization
\end{itemize}

Each of these steps are described in the following sections.

\subsection{Preprocessing}
In this step, the plane instance segmentation mask is obtained from the plane segmentation mask. This is passed along with the RGB image to the next step to enable detection and tracking of planar features.

Simulataneously, line features are extracted from the RGB image using LSD. Vanishing points are further extracted from the line segments using the approach described in that paper. Each plane segment is assigned a vanishing point direction based on the line segment direction. This will be used in refining the plane segment bounding box parameters described in next to next section. 

\subsection{Monocular Visual Inertial Odometry}
Although detailed description of VINS-Mono is beyond the scope of this paper, we briefly describe the steps involved in estimating the odometry from vision and IMU measurements. 
The system starts with measurement preprocessing, in which planar features are extracted and tracked, and IMU measurements between two consecutive frames are preintegrated. The initialization procedure provides all necessary values, including, pose, velocity, gravity vector, gyroscope bias, and three-dimensional (3-D) feature location, for bootstrapping
the subsequent nonlinear optimization-based VIO.

We use the estimated camera pose to transfer the 3D planar features into the world frame. These features are used in the subsequent step for estimating the plane segment parameters.

\subsection{Estimation of Plane Segment Parameters}
In this step we compute breadth, height and orientation of the 3D bouding box around each plane segment. Firstly, the 3D planar features are grouped based on their plane ids i. For each set of points Si, 
\begin{itemize}
\item Mid point of points in the set is computed. Also, maximum and minimum z-coordinates in the point set are found.
\item Now ignoring the z-coordinate of each point, farthest point from the mid point is found. 
\item Breadth of the plane segment is computed as, twice the distance of farthest point from the mid point.
\item Direction vector from farthest point to mid point, gives the direction of the plane. One of the horizontal vanishing point directions, which makes the smallest angle with the plane direction is assigned to the plane segment. This also means that the plane normal is along the other horizontal vanishing point direction.
\item Height of the plane segment is computed as the absolute difference between zmax and zmin computed in step 1.
\end{itemize}

The parameters computed above, are used to compute the TSDF values.

\subsection{TSDF computation}
Before we describe our approach of computing TSDFs for plane segments, we briefly present for completeness, the standard approach of computing TSDFs from sensor data.

TSDFs are constructed out of pointcloud data by raycasting points in a sensor pointcloud into a global map, then averaging the new projective distance measurements into existing voxels, calculating distances only up to a truncation distance of Î´. A common strategy to integrate a new scan or a dense point cloud into a TSDF is to ray-cast from the sensor origin to every point in the sensor data, and update the distance and weight estimates of voxels along this ray. The choice of weighting function can have a strong impact on the accuracy of the resulting reconstruction, especially for large voxels, where thousands of points may be merged into the same voxel per scan.

The general equations governing the merging are based on the existing distance and weight values of a voxel, D and W, and the new update values from a specific point observation
in the sensor, d and w, where d is the distance from the surface boundary. Given that x is the center position of the current voxel, p is the position of a 3D point in the incoming sensor data, s is the sensor origin, and x, p, s $\epsilon$ R 3 , the updated D distance and W weight values of a voxel at x will be: INSERT THE EQUATIONS HERE!




Unlike traditional approaches that use dense point clouds, we use geometry based distance functions to compute the TSDF values. Plane segment is modelled as a cuboid of infinitesimal width. The distance function of a cuboid is given by (INSERT EQUATIONS HERE!). This gives the smallest distance to the cuboid surface from point P.



However, this assumes that the point P is defined in the frame of the cuboid with its center as the origin. Therefore, we transform the point P into cuboid centric frame.

The TSDF values thus computed can be directly used to obtain the ESDF values and also to generate 3D mesh. This approach overcomes the computationally intensive steps weighing merging required in general.

We then generate the ESDFs, essential for navigation, from the TSDFs following approach described in voxblox paper.

The general algorithm is based on the idea of wavefronts â waves that propagate from a start voxel to its neighbors (using 26-connectivity), updating their distances, and putting
updated voxels into the wavefront queue to further propagate to their neighbors. Two wavefronts: raise and lower are used. A voxel gets added to the raise queue when its new distance value from the TSDF is higher than the previous value stored in the ESDF voxel. This means the voxel, and all its children, need to be invalidated. The wavefront propagates until no voxels are left with parents that have been invalidated. The lower wavefront starts when a new fixed voxel enters the map, or a previously observed voxel lowers its value. The distances of neighboring voxels get updated based on neighbor voxels and their distances to the current voxel. The wavefront ends when there are no voxels left whose distance could decrease from its neighbors.

For visualization of the reconstruction, a 3D mesh of the environment is generated from the TSDF voxel map using marching cubes algorithm.

The results of mesh and ESDF slice computed using our approach is reported in Figure 1(c) and 1 (d) respectively.

\section{Results}
We demonstrate using that using sparse planar features alone are sufficient to compute the TSDF voxel map and generate dense human-readable meshes. All the evaluations are run on a 4-core Intel Core i5-7200 CPU with 8 GB RAM. To account for randomness from RANSAC and the multi-tasking OS, we report the median results from five runs for each evaluation.

\subsection{Photorealistic synthetic dataset}
We build a custom outdoor urban environment with buildings and streets in Unreal Engine [39]. We borrow several high-quality and feature-rich assets from the FlightGoggles [40] project for photorealism. This environment is integrated with AirSim [41] to spawn a quadrotor and collect visual-inertial data. We collect monocular RGB images and their plane segmentation masks at 20 Hz, IMU measurements and ground truth poses at 200 Hz. The camera and IMU intrinsics, and the camera-IMU spatial transform are obtained directly from AirSim.

The quadrotor is controlled to move along the streets, while moving along a sine wave in the vertical direction, resulting in a sinusoidal pattern. This sinusoidal excitation
along the height is to ensure a non-constant acceleration and keep the scale observable [4]. We further command it to accelerate vertically at the beginning of its motion,
before following the trajectory, to help the initialization. The total trajectory is of 200 m length and 80 s duration, with a maximum speed of 3 m/s. The quadrotor is also controlled programmatically, such that the buildings are in the field-of-view of the camera for the entire sequence.


Our setup is described as follows:
We created a 3D model of an urban environment in unreal engine. It consists of so and so many number of buildings of different textures. Using AirSim, a virtual drone is moved along a sinusoidal trajectory in the environment as shown in the figure.
A sinusoidal trajectory is traversed to capture the sequence of images and IMU data. The recorded sequence is for quantitative and qualitative evaluation.

\begin{figure}[h!]
  \centering
  \includegraphics[width=\linewidth]{images/c1_image_80.png}
  \caption{Photorealistic synthetic dataset created to evaluate our approach}
\end{figure}

\subsection{Evaluation}
We choose the standard TSDF computation flow described in section 3.4 as the baseline for evaluation of our method. 
fig shows the comparison of the obtained reconstruction with and without vanishing point constraints. We compare the computation time required by our w.r.t the baseline in the table with comparision. As mentioned in section 3, the direct computation of SDF values from plane segment parameters overcomes the need for dense point clouds as needed by the baseline approach.

We reconstruct the planar facades of the buildings in the dataset. We report the obtained reconstructions from baseline approach  qualitative evaluation. We also present the effect of using vanishing point constraints on the reconstruction.

\begin{figure*}
  \centering
  \subcaptionbox{VINS-Mono}{
    \includegraphics[width=0.45\linewidth]{images/results/vins_mono_recon.png}
  }
  \subcaptionbox{voxblox}{
    \includegraphics[width=0.45\linewidth]{images/results/voxblox_recon.png}
  }
  \subcaptionbox{Ours (without VP constraints)}{
    \includegraphics[width=0.45\linewidth]{images/results/ours_without_vp_recon.png}
  }
  \subcaptionbox{Ours (with VP constraints)}{
    \includegraphics[width=0.45\linewidth]{images/results/our_recon.png}
  }
  \caption{Qualitative comparision of reconstructions. Figure (a) shows VINS-Mono's planar features, which is a sparse map and the estimated odometry. Green-Estimated trajectory. Red-Ground truth trajectory obtained from AirSim. Figures (b)-(d) shows the reconstructions of skylines using three approaches viz., voxblox using sparse features from VINS-Mono, Ours (without vanishing point constraints), Ours with vanishing point constraints. In each case, 3D mesh is generated from computed TSDF voxel map. A slice of ESDF voxel map is also shown. Our approach with vanishing point constraints reconstructs the scene inconsistent with the geometry of the environment.}
\end{figure*}

\subsection{Discussion}
Qualitatively our method produces a dense TSDF voxel map, as visible in the reported ESDF slices. The quality of meshes generated by our method is superior that of the baseline even with the use of sparse feature map. Owing to the use of vanishing point information, the ESDF slices also capture the topological layout of the environment.

\section{Conclusion and Future Work}
In this paper we showed that the sparse planar features in the monocular UAV images can be used to directly compute the TSDF values and thereby ESDF voxel map. The quality of generated meshses and the dense ESDF slices further confirm their usefulness for downstream tasks such as obstacle avoidance and path planning.

As a natural extension of this work, we would like to evaluate the proposed approach on real world datasets. In future, we plan to evaluate the performance of our method by a tightly-coupling the state estimation and mapping inorder to impose the topological constraints in the factor graph.